{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275bb4e5-fe2a-464c-a92b-c9aa9b3ef7f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __CONVERTING NEXT DAY WILDFIRE DATASET TF RECORDS TO CSV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fccc665e-6a27-4be9-8ddd-eade59867c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas_tfrecords as pdtfr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1534fe4b-f2b5-472d-86a4-80604102bc91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_day_wildfire_spread_train_03.tfrecord\n",
      "next_day_wildfire_spread_train_09.tfrecord\n",
      "next_day_wildfire_spread_train_14.tfrecord\n",
      "next_day_wildfire_spread_train_11.tfrecord\n",
      "next_day_wildfire_spread_train_04.tfrecord\n",
      "next_day_wildfire_spread_test_01.tfrecord\n",
      "next_day_wildfire_spread_train_10.tfrecord\n",
      "next_day_wildfire_spread_train_08.tfrecord\n",
      "next_day_wildfire_spread_eval_00.tfrecord\n",
      "next_day_wildfire_spread_train_06.tfrecord\n",
      "next_day_wildfire_spread_train_00.tfrecord\n",
      "next_day_wildfire_spread_train_02.tfrecord\n",
      "next_day_wildfire_spread_train_05.tfrecord\n",
      "next_day_wildfire_spread_eval_01.tfrecord\n",
      "next_day_wildfire_spread_train_01.tfrecord\n",
      "next_day_wildfire_spread_train_07.tfrecord\n",
      "next_day_wildfire_spread_train_13.tfrecord\n",
      "next_day_wildfire_spread_test_00.tfrecord\n",
      "next_day_wildfire_spread_train_12.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# checking filenames and setting dataset path\n",
    "dataset_path = '../../datasets/next_day_wildfire/'\n",
    "\n",
    "for subdirs, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "867bb9e2-6602-4f03-85b0-1de67811b739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f_parser(tfrecord_file):\n",
    "    # Replace 'your_file.tfrecord' with your actual TFRecord file.\n",
    "    # tfrecord_file = 'next_day_wildfire_spread_eval_00.tfrecord'\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "\n",
    "    # Create an empty list to hold the data.\n",
    "    data = []\n",
    "\n",
    "    # Initialize a list to store the column names.\n",
    "    headings = None\n",
    "\n",
    "    # Iterate through the TFRecord file.\n",
    "    for record in dataset:\n",
    "        example = tf.train.Example.FromString(record.numpy())\n",
    "        \n",
    "        # Assuming that the first record contains column names.\n",
    "        if headings is None:\n",
    "            headings = list(example.features.feature.keys())\n",
    "        \n",
    "        # Initialize a dictionary for the current example.\n",
    "        current_data = {}\n",
    "        \n",
    "        # Iterate through the features and handle their types.\n",
    "        for key, feature in example.features.feature.items():\n",
    "            if feature.HasField('bytes_list'):\n",
    "                # Handle string data.\n",
    "                current_data[key] = feature.bytes_list.value[0].decode('utf-8')\n",
    "            elif feature.HasField('int64_list'):\n",
    "                # Handle integer data.\n",
    "                current_data[key] = feature.int64_list.value[0]\n",
    "            elif feature.HasField('float_list'):\n",
    "                # Handle float data.\n",
    "                current_data[key] = feature.float_list.value[0]\n",
    "        \n",
    "        # Append the data for the current example.\n",
    "        data.append(current_data)\n",
    "\n",
    "    # Convert the data into a NumPy array.\n",
    "    numpy_array = np.array(data)\n",
    "\n",
    "    # # Print the column names (headings) and the first row of data.\n",
    "    # print(headings)\n",
    "    # print(list(numpy_array[0].values()))\n",
    "    # print(numpy_array.shape)\n",
    "\n",
    "    # Save the NumPy array as a CSV file\n",
    "    file_name = tfrecord_file.replace(\".tfrecord\", \"\") + '.csv' \n",
    "    csv_filename = file_name.split(\"/\")\n",
    "    csv_filename = csv_filename[-1]\n",
    "    # CSV Dataset Path Added\n",
    "    \n",
    "    \n",
    "    np.savetxt(\"../../datasets/next_day_wildfire_csv/\" + csv_filename, numpy_array, delimiter=',', fmt='%s')\n",
    "    return csv_filename , headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5aeb9be0-b448-4c61-979b-58dcc6f3b9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '../../datasets/next_day_wildfire_csv/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../datasets/next_day_wildfire_csv/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subdirs, dirs, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(dataset_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# This function will gene\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# if (file[-4:] == \".csv\"):\u001b[39;00m\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;66;03m# os.remove(dataset_path + file)\u001b[39;00m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '../../datasets/next_day_wildfire_csv/'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"../../datasets/next_day_wildfire_csv/\")\n",
    "\n",
    "for subdirs, dirs, files in os.walk(dataset_path):\n",
    "    # This function will gene\n",
    "    for file in files:\n",
    "        # if (file[-4:] == \".csv\"):\n",
    "            # os.remove(dataset_path + file)\n",
    "        csv_filename, headings = f_parser(dataset_path + file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "daf34226-e76e-4514-9d3a-aa1a6d8e9c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmmn', 'NDVI', 'elevation', 'population', 'FireMask', 'pdsi', 'vs', 'pr', 'tmmx', 'sph', 'th', 'PrevFireMask', 'erc']\n"
     ]
    }
   ],
   "source": [
    "print(headings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
