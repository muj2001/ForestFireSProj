{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275bb4e5-fe2a-464c-a92b-c9aa9b3ef7f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __CONVERTING NEXT DAY WILDFIRE DATASET TF RECORDS TO CSV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fccc665e-6a27-4be9-8ddd-eade59867c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas_tfrecords as pdtfr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1534fe4b-f2b5-472d-86a4-80604102bc91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_day_wildfire_spread_train_03.tfrecord\n",
      "next_day_wildfire_spread_train_09.tfrecord\n",
      "next_day_wildfire_spread_train_14.tfrecord\n",
      "next_day_wildfire_spread_train_11.tfrecord\n",
      "next_day_wildfire_spread_train_04.tfrecord\n",
      "next_day_wildfire_spread_test_01.tfrecord\n",
      "next_day_wildfire_spread_train_10.tfrecord\n",
      "next_day_wildfire_spread_train_08.tfrecord\n",
      "next_day_wildfire_spread_eval_00.tfrecord\n",
      "next_day_wildfire_spread_train_06.tfrecord\n",
      "next_day_wildfire_spread_train_00.tfrecord\n",
      "next_day_wildfire_spread_train_02.tfrecord\n",
      "next_day_wildfire_spread_train_05.tfrecord\n",
      "next_day_wildfire_spread_eval_01.tfrecord\n",
      "next_day_wildfire_spread_train_01.tfrecord\n",
      "next_day_wildfire_spread_train_07.tfrecord\n",
      "next_day_wildfire_spread_train_13.tfrecord\n",
      "next_day_wildfire_spread_test_00.tfrecord\n",
      "next_day_wildfire_spread_train_12.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# checking filenames and setting dataset path\n",
    "dataset_path = '../../datasets/next_day_wildfire/'\n",
    "\n",
    "save_path = \"../../datasets/next_day_wildfire_csv/\"\n",
    "\n",
    "for subdirs, dirs, files in os.walk(dataset_path):\n",
    "    for file in files:\n",
    "      print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ad0610e-37e2-44a1-b716-bc0c98f123e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_headings(tfrecord_file):\n",
    "    # Replace 'your_file.tfrecord' with your actual TFRecord file.\n",
    "    # tfrecord_file = 'next_day_wildfire_spread_eval_00.tfrecord'\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "    \n",
    "    headings = None\n",
    "    \n",
    "    for record in dataset:\n",
    "        example = tf.train.Example.FromString(record.numpy())\n",
    "        \n",
    "        # Assuming that the first record contains column names.\n",
    "        if headings is None:\n",
    "            headings = list(example.features.feature.keys())\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return headings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "867bb9e2-6602-4f03-85b0-1de67811b739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f_parser(tfrecord_file, headings):\n",
    "    # Replace 'your_file.tfrecord' with your actual TFRecord file.\n",
    "    # tfrecord_file = 'next_day_wildfire_spread_eval_00.tfrecord'\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "\n",
    "    # Create an empty list to hold the data.\n",
    "    data = []\n",
    "    \n",
    "    data.append(headings)\n",
    "    \n",
    "    # Initialize a list to store the column names.\n",
    "\n",
    "    # Iterate through the TFRecord file.\n",
    "    for record in dataset:\n",
    "        example = tf.train.Example.FromString(record.numpy())\n",
    "        \n",
    "        current_data = {}\n",
    "        \n",
    "        # Iterate through the features and handle their types.\n",
    "        for key, feature in example.features.feature.items():\n",
    "            if feature.HasField('bytes_list'):\n",
    "                # Handle string data.\n",
    "                current_data[key] = feature.bytes_list.value[0].decode('utf-8')\n",
    "            elif feature.HasField('int64_list'):\n",
    "                # Handle integer data.\n",
    "                current_data[key] = feature.int64_list.value[0]\n",
    "            elif feature.HasField('float_list'):\n",
    "                # Handle float data.\n",
    "                current_data[key] = feature.float_list.value[0]\n",
    "        \n",
    "        current_example = []\n",
    "        for heading in headings:\n",
    "            current_example.append(current_data[heading])\n",
    "        \n",
    "        # Append the data for the current example.\n",
    "        data.append(current_example)\n",
    "\n",
    "    # Save the data as a CSV file\n",
    "    file_name = tfrecord_file.replace(\".tfrecord\", \"\") + '.csv' \n",
    "    csv_filename = file_name.split(\"/\")\n",
    "    csv_filename = csv_filename[-1]\n",
    "    # CSV Dataset Path Added\n",
    "    \n",
    "    \n",
    "    np.savetxt(save_path + csv_filename, data, delimiter=',', fmt='%s')\n",
    "    return csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a409def4-b1bd-4735-856c-6d298fb9e811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NDVI', 'tmmn', 'elevation', 'population', 'FireMask', 'vs', 'pdsi', 'pr', 'tmmx', 'sph', 'th', 'PrevFireMask', 'erc']\n",
      "['FireMask', 'NDVI', 'PrevFireMask', 'elevation', 'erc', 'pdsi', 'population', 'pr', 'sph', 'th', 'tmmn', 'tmmx', 'vs']\n"
     ]
    }
   ],
   "source": [
    "headings = None\n",
    "\n",
    "headings = add_headings(dataset_path + \"next_day_wildfire_spread_train_00.tfrecord\")\n",
    "\n",
    "print(headings)\n",
    "\n",
    "headings = sorted(headings)\n",
    "\n",
    "print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5aeb9be0-b448-4c61-979b-58dcc6f3b9f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.mkdir(save_path)\n",
    "\n",
    "# print(\"HEADINGS\", headings)\n",
    "\n",
    "for subdirs, dirs, files in os.walk(dataset_path):\n",
    "    # This function will gene\n",
    "    for file in files:\n",
    "        # if (file[-4:] == \".csv\"):\n",
    "            # os.remove(dataset_path + file)\n",
    "        csv_filename = f_parser(dataset_path + file, headings)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "daf34226-e76e-4514-9d3a-aa1a6d8e9c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tmmn', 'NDVI', 'elevation', 'population', 'FireMask', 'pdsi', 'vs', 'pr', 'tmmx', 'sph', 'th', 'PrevFireMask', 'erc']\n"
     ]
    }
   ],
   "source": [
    "print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41b76fea-e194-4651-8934-2688fbe64048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### WARNING ###\n",
    "# THIS CELL IS USED TO DELETE THE CSV FOLDER #\n",
    "\n",
    "for subdirs, dirs, files in os.walk(save_path):\n",
    "    # This function will gene\n",
    "    for file in files:\n",
    "        os.remove(save_path + file)\n",
    "\n",
    "os.rmdir(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
